{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "yWsOByAG_eoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hnJLfKi4uBG"
      },
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Text preprocessing\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Handling imbalanced datasets\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "# Model selection and preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.ensemble import (VotingClassifier, StackingClassifier,\n",
        "                              RandomForestClassifier, GradientBoostingClassifier,\n",
        "                              AdaBoostClassifier, ExtraTreesClassifier)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier  # Ensure you have xgboost installed\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, f1_score,\n",
        "                             recall_score, precision_score,\n",
        "                             mean_absolute_error, mean_squared_error,\n",
        "                             roc_curve, roc_auc_score)\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TensorFlow/Keras models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Flatten,\n",
        "                                     Dense, Dropout, GlobalAveragePooling1D,\n",
        "                                     BatchNormalization, Add, Bidirectional, LSTM,\n",
        "                                     Attention)\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "d1C9pf74_j0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "data = pd.read_csv('avped-dataset/AntiVped.csv')\n",
        "data.info()"
      ],
      "metadata": {
        "id": "EilhnUHZ_nYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing and Splitting"
      ],
      "metadata": {
        "id": "mSJ6TS3eBH3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert non-numeric values to NaN for numeric columns\n",
        "numeric_columns = ['molecular \\nweight', 'Theoretical Pi', 'Extinction \\ncoefficient',\n",
        "                   'Half life\\n(hours)', 'Instability\\n index', 'Aliphatic\\n index',\n",
        "                   'GRAVY', 'Hydrophobic \\nresidue :', 'Net charge:',\n",
        "                   'Boman Index:\\n(Kcal/mol)', 'Protective Antigenic\\n Score']\n",
        "\n",
        "data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill missing values for numerical columns with their mean\n",
        "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
        "\n",
        "# Fill missing values for the categorical column 'Allergenicity' with the most frequent value\n",
        "data['Allergenicity'] = data['Allergenicity'].fillna(data['Allergenicity'].mode()[0])\n",
        "\n",
        "# Clean up the column names\n",
        "data.columns = data.columns.str.replace('\\n', '').str.strip()\n",
        "\n",
        "# Encode the 'Allergenicity' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "data['Allergenicity'] = label_encoder.fit_transform(data['Allergenicity'])\n",
        "\n",
        "# Get unique amino acids in the dataset and create encoding\n",
        "all_amino_acids = set(''.join(data['Sequence'].tolist()))\n",
        "amino_acid_to_index = {amino_acid: i for i, amino_acid in enumerate(all_amino_acids)}\n",
        "\n",
        "# Preprocess sequence data\n",
        "sequences_encoded = []\n",
        "for sequence in data['Sequence']:\n",
        "    encoded_sequence = [amino_acid_to_index[amino_acid] for amino_acid in sequence]\n",
        "    sequences_encoded.append(encoded_sequence)\n",
        "\n",
        "# Pad the sequences\n",
        "max_sequence_length = max(len(seq) for seq in sequences_encoded)\n",
        "sequences_padded = pad_sequences(sequences_encoded, maxlen=max_sequence_length, padding='post')\n",
        "data['Sequence_Encoded'] = sequences_padded.tolist()\n",
        "\n",
        "# Ensure the correct target column name here\n",
        "y = data['property = 1']  # Replace with the actual name of the target column\n",
        "\n",
        "# Select the features for X\n",
        "X = data[['Sequence_Encoded', 'molecular weight', 'Extinction coefficient', 'Theoretical Pi',\n",
        "          'Half life(hours)', 'Instability index', 'Aliphatic index', 'GRAVY',\n",
        "          'Hydrophobic residue :', 'Net charge:', 'Boman Index:(Kcal/mol)',\n",
        "          'Protective Antigenic Score', 'Allergenicity']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert sequence data to numpy arrays\n",
        "X_train_sequences = np.array(X_train['Sequence_Encoded'].tolist())\n",
        "X_test_sequences = np.array(X_test['Sequence_Encoded'].tolist())\n",
        "\n",
        "# Flatten the sequence data\n",
        "X_train_sequences_flat = X_train_sequences.reshape(X_train_sequences.shape[0], -1)\n",
        "X_test_sequences_flat = X_test_sequences.reshape(X_test_sequences.shape[0], -1)\n",
        "\n",
        "# Combine the flattened sequence data with other features\n",
        "X_train_combined = np.hstack((X_train_sequences_flat, X_train.drop(columns=['Sequence_Encoded']).values))\n",
        "X_test_combined = np.hstack((X_test_sequences_flat, X_test.drop(columns=['Sequence_Encoded']).values))\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_combined, y_train)"
      ],
      "metadata": {
        "id": "A08wZW69ATTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Distribution"
      ],
      "metadata": {
        "id": "zFn_1Q_-AYkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = y_train.value_counts()\n",
        "\n",
        "print(class_counts)\n",
        "\n",
        "beingsaved = plt.figure(figsize=(4, 4))\n",
        "colors = ['#66b3ff', '#99ff99']\n",
        "plt.bar(['AVP', 'Non-AVP'], class_counts.values, color=colors)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Class Distribution')\n",
        "beingsaved.savefig('/kaggle/working/Class_Distribution.png', format='png', dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0-nD5W8AYOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Techniques"
      ],
      "metadata": {
        "id": "VtynMFF1AhGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New ensemble classifiers\n",
        "ensemble1 = VotingClassifier(estimators=[\n",
        "    ('gb',GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=10,\n",
        "                                       min_samples_split=2, min_samples_leaf=1,\n",
        "                                       random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2,\n",
        "                                   min_samples_leaf=1, max_features='sqrt',\n",
        "                                   random_state=42)),\n",
        "    ('et', KNeighborsClassifier(\n",
        "    n_neighbors=5,         # Number of neighbors to use for knearest neighbors\n",
        "    weights='uniform',     # Weight function used in prediction ('uniform' or 'distance')\n",
        "    algorithm='auto',      # Algorithm used to compute the nearest neighbors ('auto', 'ball_tree', 'kd_tree', 'brute')\n",
        "    leaf_size=30,          # Leaf size passed to the underlying tree-based algorithms\n",
        "    metric='minkowski'     # Distance metric to use for the tree (default is Minkowski)             # Seed for reproducibility\n",
        ")),\n",
        "    ('ada', AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=42 ))\n",
        "], voting='soft')\n",
        "\n",
        "ensemble2 = StackingClassifier(estimators=[\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=10,\n",
        "                                       min_samples_split=2, min_samples_leaf=1,\n",
        "                                       random_state=42)),\n",
        "    ('et', KNeighborsClassifier(\n",
        "    n_neighbors=5,         # Number of neighbors to use for knearest neighbors\n",
        "    weights='uniform',     # Weight function used in prediction ('uniform' or 'distance')\n",
        "    algorithm='auto',      # Algorithm used to compute the nearest neighbors ('auto', 'ball_tree', 'kd_tree', 'brute')\n",
        "    leaf_size=30,          # Leaf size passed to the underlying tree-based algorithms\n",
        "    metric='minkowski'     # Distance metric to use for the tree (default is Minkowski)\n",
        ")),\n",
        "    ('ada', AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=42 ))\n",
        "], final_estimator=RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2,\n",
        "                                   min_samples_leaf=1, max_features='sqrt',\n",
        "                                   random_state=42))\n",
        "\n",
        "ensemble3 = VotingClassifier(estimators=[\n",
        "    ('xgb', XGBClassifier(use_label_encoder=True, eval_metric='logloss', n_estimators=200,\n",
        "                           max_depth=10, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
        "                           gamma=0, min_child_weight=1)),\n",
        "    ('brf', BalancedRandomForestClassifier(\n",
        "        n_estimators=200, criterion='gini', max_depth=10,\n",
        "        min_samples_split=2, min_samples_leaf=1,\n",
        "        min_weight_fraction_leaf=0.0, bootstrap=True,\n",
        "        oob_score=False, random_state=42,\n",
        "        verbose=0, class_weight='balanced',\n",
        "        min_impurity_decrease=0.0)),\n",
        "    ('et', KNeighborsClassifier(\n",
        "    n_neighbors=5,         # Number of neighbors to use for knearest neighbors\n",
        "    weights='uniform',     # Weight function used in prediction ('uniform' or 'distance')\n",
        "    algorithm='auto',      # Algorithm used to compute the nearest neighbors ('auto', 'ball_tree', 'kd_tree', 'brute')\n",
        "    leaf_size=30,          # Leaf size passed to the underlying tree-based algorithms\n",
        "    metric='minkowski'     # Distance metric to use for the tree (default is Minkowski)\n",
        ")),\n",
        "    ('ada', AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=42 ))\n",
        "], voting='soft')\n",
        "\n",
        "ensemble4 = StackingClassifier(estimators=[\n",
        "    ('xgb', XGBClassifier(use_label_encoder=True, eval_metric='logloss', n_estimators=200,\n",
        "                           max_depth=10, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,\n",
        "                           gamma=0, min_child_weight=1)),\n",
        "    ('et', KNeighborsClassifier(\n",
        "    n_neighbors=5,         # Number of neighbors to use for knearest neighbors\n",
        "    weights='uniform',     # Weight function used in prediction ('uniform' or 'distance')\n",
        "    algorithm='auto',      # Algorithm used to compute the nearest neighbors ('auto', 'ball_tree', 'kd_tree', 'brute')\n",
        "    leaf_size=30,          # Leaf size passed to the underlying tree-based algorithms\n",
        "    metric='minkowski'     # Distance metric to use for the tree (default is Minkowski)\n",
        ")),\n",
        "    ('ada', AdaBoostClassifier(n_estimators=200, learning_rate=0.1, random_state=42 ))\n",
        "], final_estimator=BalancedRandomForestClassifier(n_estimators=200, criterion='gini', max_depth=10,\n",
        "        min_samples_split=2, min_samples_leaf=1,\n",
        "        min_weight_fraction_leaf=0.0, bootstrap=True,\n",
        "        oob_score=False, random_state=42,\n",
        "        verbose=0, class_weight='balanced',\n",
        "        min_impurity_decrease=0.0))\n",
        "\n",
        "\n",
        "Add this to your classifiers dictionary\n",
        "classifiers = {\n",
        "    'SVM': SVC(probability=True, C=1.0, gamma='scale',\n",
        "               shrinking=True, tol=1e-3, cache_size=200, verbose=False, max_iter=-1,\n",
        "               decision_function_shape='ovr', break_ties=False),\n",
        "\n",
        "    'Decision Tree': DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=5,\n",
        "                                            min_samples_split=2, min_samples_leaf=1),\n",
        "\n",
        "    'Extra Tree': ExtraTreesClassifier(n_estimators=100, max_depth=5),\n",
        "\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=2,\n",
        "                                   min_samples_leaf=1, random_state=42),\n",
        "\n",
        "    'Logistic Regression': LogisticRegression(penalty='l2', dual=False, tol=0.0001,\n",
        "                                             C=1.0, fit_intercept=True,\n",
        "                                             intercept_scaling=1,\n",
        "                                             max_iter=100,\n",
        "                                             verbose=0, warm_start=False),\n",
        "\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5,\n",
        "                                       min_samples_split=2, min_samples_leaf=1),\n",
        "\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5, leaf_size=30),\n",
        "\n",
        "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), activation='relu',\n",
        "                                    solver='adam', alpha=0.0001, batch_size='auto',\n",
        "                                    learning_rate='constant', learning_rate_init=0.001,\n",
        "                                    power_t=0.5, max_iter=200, shuffle=True,\n",
        "                                    random_state=None, tol=0.0001,\n",
        "                                    verbose=False, warm_start=False,\n",
        "                                    momentum=0.9, nesterovs_momentum=True,\n",
        "                                    early_stopping=False, validation_fraction=0.1,\n",
        "                                    beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "\n",
        "    'Naive Bayes': GaussianNB(priors=None, var_smoothing=1e-09),\n",
        "\n",
        "    'AdaBoost': AdaBoostClassifier(n_estimators=100),\n",
        "\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100),\n",
        "\n",
        "    'Balanced RF': BalancedRandomForestClassifier(\n",
        "        n_estimators=100, random_state=42),\n",
        "\n",
        "    'Proposed Ensemble 1': ensemble1,\n",
        "    'Proposed Ensemble 2': ensemble2,\n",
        "    'Proposed Ensemble 3': ensemble3,\n",
        "    'Proposed Ensemble 4': ensemble4\n",
        "}"
      ],
      "metadata": {
        "id": "BuP-R8tPApz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "REX1ZZAFA9Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists for ROC data\n",
        "roc_data = {}\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "for name, classifier in classifiers.items():\n",
        "    print(f\"Model: {name}\")\n",
        "\n",
        "    classifier.fit(X_train_balanced, y_train_balanced)\n",
        "    y_pred = classifier.predict(X_test_combined)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate F1 score, precision, recall, etc.\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    # Calculate Specificity\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    # Calculate Matthews Correlation Coefficient\n",
        "    mcc = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5 if (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) > 0 else 0\n",
        "\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Mean Absolute Error: {mae}\")\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "    print(f\"ROC AUC Score: {roc_auc}\")\n",
        "    print(f\"Specificity: {specificity}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc}\")\n",
        "\n",
        "    # Create and save a heatmap of the confusion matrix\n",
        "    plt.figure()\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=True,\n",
        "                xticklabels=[\"AVP\", \"Non-AVP\"], yticklabels=[\"AVP\", \"Non-AVP\"])\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"Actual Labels\")\n",
        "    plt.title(f\"Confusion Matrix for {name}\")\n",
        "\n",
        "    # Save the confusion matrix as an image file\n",
        "    plt.savefig(f'Confusion_{name}.png', format='png', dpi=600, bbox_inches=\"tight\")\n",
        "    plt.show()  # Close the plot to avoid display\n",
        "\n",
        "    # Store ROC curve data\n",
        "    y_prob = classifier.predict_proba(X_test_combined)[:, 1]  # Probability estimates for the positive class\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "    # Ensure ROC curve starts at (0, 0)\n",
        "    fpr = np.concatenate([[0], fpr])  # Add 0 at the start\n",
        "    tpr = np.concatenate([[0], tpr])  # Add 0 at the start\n",
        "    roc_data[name] = (fpr, tpr, roc_auc)\n",
        "\n",
        "    print(\"---------------------------------------\")"
      ],
      "metadata": {
        "id": "ZdteuDqwA6lk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}